{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czr/anaconda3/envs/ad/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**A Language Agent for Autonomous Driving**\n",
      "Role: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to extract necessary information from the driving scenario. The information you extracted must be useful to the next-step motion planning. \n",
      "\n",
      "Necessary information might include the following:\n",
      "- Detections: The detected objects that you need to pay attention to.\n",
      "- Predictions: The estimated future motions of the detected objects. \n",
      "- Maps: Map information includes traffic lanes and road boundaries.\n",
      "- Occunpancy: Occupancy implies whether a location has been occupied by other objects.\n",
      "\n",
      "Task\n",
      "- You should think about what types of information (Detections, Predictions, Maps, Occupancy) you need to extract from the driving scenario.\n",
      "- Detections and Predictions are quite important for motion planning. You should call at least one of them if necessary.\n",
      "- Maps information are also important. You should pay more attention to road shoulder and lane divider information to your current ego-vehicle location.\n",
      "- I will guide you through the thinking process step by step.\n",
      "\n",
      "*****Ego States:*****\n",
      "Current State:\n",
      " - Velocity (vx,vy): (-0.01,0.92)\n",
      " - Heading Angular Velocity (v_yaw): (0.00)\n",
      " - Acceleration (ax,ay): (-0.00,-0.50)\n",
      " - Can Bus: (-0.74,0.14)\n",
      " - Heading Speed: (0.95)\n",
      " - Steering: (-0.02)\n",
      "Historical Trajectory (last 2 seconds): [(-0.07,-6.43), (-0.05,-4.34), (-0.02,-2.32), (-0.01,-0.91)]\n",
      "Mission Goal: FORWARD\n",
      "\n",
      "\n",
      "\n",
      "Do you need to perform detections from the driving scenario?\n",
      "Please answer YES or NO.\n",
      "\n",
      "YES\n",
      "\n",
      "        You can execute one of the following functions to get object detection results \n",
      "        (don't execute functions that have been used before):\n",
      "\n",
      "        - get_leading_object_detection() #Get the detection of the leading object, the function will return the leading object id and its position and size. If there is no leading object, return None\n",
      "- get_object_detections_in_range(x_start, x_end, y_start, y_end) #Get the detections of the objects in a given range (x_start, x_end)*(y_start, y_end)m^2, the function will return a list of object ids and their positions and sizes. If there is no object, return None\n",
      "- get_surrounding_object_detections() #Get the detections of the surrounding objects in a 20m*20m range, the function will return a list of surroundind object ids and their positions and sizes. If there is no surrounding object, return None\n",
      "- get_front_object_detections() #Get the detections of the objects in front of you in a 10m*20m range, the function will return a list of front object ids and their positions and sizes. If there is no front object, return None\n",
      "- get_all_object_detections() #Get the detections of all objects in the whole scene, the function will return a list of object ids and their positions and sizes. Always avoid using this function if there are other choices.\n",
      "\n",
      "None\n",
      "\u001b[41mDetection Function Call: get_leading_object_detection\u001b[0m\n",
      "get_leading_object_detection\n",
      "{}\n",
      "\n",
      "function_response {'name': 'get_leading_object_detection', 'args': {}, 'prompt': '', 'data': []}\n",
      "\n",
      "Do you need to perform future trajectory predictions for the detected objects?\n",
      "Please answer YES or NO.\n",
      "\n",
      "YES\n",
      "You can execute one of the following functions to get object future trajectory predictions (don't execute functions that have been used before):\n",
      "- get_leading_object_future_trajectory() #Get the predicted future trajectory of the leading object, the function will return a trajectory containing a series of waypoints. If there is no leading vehicle, return None\n",
      "- get_future_trajectories_for_specific_objects(object_ids) #Get the future trajectories of specific objects (specified by a List of object ids), the function will return trajectories for each object. If there is no object, return None\n",
      "- get_future_trajectories_in_range(x_start, x_end, y_start, y_end) #Get the future trajectories where any waypoint in this trajectory falls into a given range (x_start, x_end)*(y_start, y_end)m^2, the function will return each trajectory that satisfies the condition. If there is no trajectory satisfied, return None\n",
      "- get_future_waypoint_of_specific_objects_at_timestep(object_ids, timestep) #Get the future waypoints of specific objects at a specific timestep, the function will return a list of waypoints. If there is no object or the object does not have a waypoint at the given timestep, return None\n",
      "- get_all_future_trajectories() #Get the predicted future trajectories of all objects in the whole scene, the function will return a list of object ids and their future trajectories. Always avoid using this function if there are other choices.\n",
      "\n",
      "None\n",
      "\u001b[44mPrediction Function Call: get_future_trajectories_for_specific_objects\u001b[0m\n",
      "get_future_trajectories_for_specific_objects\n",
      "{'object_ids': [1, 2, 3]}\n",
      "Future trajectories for specific objects:\n",
      "Object type: car, object id: 1, future waypoint coordinates in 3s: [(-14.26, 8.84), (-14.25, 8.83), (-14.23, 8.82), (-14.24, 8.83), (-14.24, 8.83), (-14.22, 8.84)]\n",
      "Object type: car, object id: 2, future waypoint coordinates in 3s: [(4.36, 9.56), (4.36, 9.56), (4.36, 9.57), (4.36, 9.57), (4.36, 9.56), (4.36, 9.56)]\n",
      "Object type: car, object id: 3, future waypoint coordinates in 3s: [(-2.66, 13.82), (-1.69, 14.79), (-0.99, 16.13), (-0.25, 17.73), (0.19, 19.42), (0.57, 21.35)]\n",
      "\n",
      "function_response {'name': 'get_future_trajectories_for_specific_objects', 'args': {'object_ids': [1, 2, 3]}, 'prompt': 'Future trajectories for specific objects:\\nObject type: car, object id: 1, future waypoint coordinates in 3s: [(-14.26, 8.84), (-14.25, 8.83), (-14.23, 8.82), (-14.24, 8.83), (-14.24, 8.83), (-14.22, 8.84)]\\nObject type: car, object id: 2, future waypoint coordinates in 3s: [(4.36, 9.56), (4.36, 9.56), (4.36, 9.57), (4.36, 9.57), (4.36, 9.56), (4.36, 9.56)]\\nObject type: car, object id: 3, future waypoint coordinates in 3s: [(-2.66, 13.82), (-1.69, 14.79), (-0.99, 16.13), (-0.25, 17.73), (0.19, 19.42), (0.57, 21.35)]\\n', 'data': [{'name': 'car', 'bbox': array([-1.4257118e+01,  8.8510628e+00, -1.8710864e+00,  1.8446120e+00,\n",
      "        4.3993731e+00,  1.7340828e+00, -1.6259596e+00,  7.6536904e-05,\n",
      "        2.7367103e-04], dtype=float32), 'traj': array([[-14.258966 ,   8.841694 ],\n",
      "       [-14.249004 ,   8.82988  ],\n",
      "       [-14.232348 ,   8.822122 ],\n",
      "       [-14.235255 ,   8.831818 ],\n",
      "       [-14.2355585,   8.832007 ],\n",
      "       [-14.223217 ,   8.8442335],\n",
      "       [-14.192293 ,   8.832363 ],\n",
      "       [-14.180158 ,   8.835132 ],\n",
      "       [-14.202284 ,   8.81402  ],\n",
      "       [-14.182863 ,   8.841075 ],\n",
      "       [-14.159714 ,   8.8240795],\n",
      "       [-14.145647 ,   8.820885 ]], dtype=float32), 'id': 1}, {'name': 'car', 'bbox': array([ 4.3576279e+00,  9.5646973e+00, -1.3577611e+00,  1.8636938e+00,\n",
      "        4.7187157e+00,  1.4994403e+00,  3.1256633e+00, -5.8352592e-04,\n",
      "        2.9314993e-04], dtype=float32), 'traj': array([[4.3574157, 9.564193 ],\n",
      "       [4.3561816, 9.563167 ],\n",
      "       [4.35601  , 9.565954 ],\n",
      "       [4.3573565, 9.565505 ],\n",
      "       [4.3586326, 9.562102 ],\n",
      "       [4.3551526, 9.564755 ],\n",
      "       [4.352745 , 9.56019  ],\n",
      "       [4.3559237, 9.558595 ],\n",
      "       [4.3577557, 9.557987 ],\n",
      "       [4.357852 , 9.557873 ],\n",
      "       [4.362437 , 9.56058  ],\n",
      "       [4.356148 , 9.558936 ]], dtype=float32), 'id': 2}, {'name': 'car', 'bbox': array([-3.6994514, 13.0845375, -1.2927334,  2.0073702,  4.9225225,\n",
      "        1.7195722, -1.951627 ,  2.3775587,  1.4927293], dtype=float32), 'traj': array([[-2.6573467 , 13.821069  ],\n",
      "       [-1.6896491 , 14.792089  ],\n",
      "       [-0.9853103 , 16.134518  ],\n",
      "       [-0.24887705, 17.727306  ],\n",
      "       [ 0.19207835, 19.421175  ],\n",
      "       [ 0.5712905 , 21.349163  ],\n",
      "       [ 0.7816105 , 23.438171  ],\n",
      "       [ 0.8673272 , 25.837482  ],\n",
      "       [ 0.8177624 , 28.12053   ],\n",
      "       [ 0.5323305 , 30.611012  ],\n",
      "       [ 0.24987507, 32.89445   ],\n",
      "       [-0.14874053, 34.90399   ]], dtype=float32), 'id': 3}]}\n",
      "\n",
      "Do you need to get occupancy information for this driving scenario?\n",
      "Please answer YES or NO.\n",
      "\n",
      "NO\n",
      "\n",
      "Do you need to get map information for this driving scenario?\n",
      "Please answer YES or NO.\n",
      "\n",
      "YES\n",
      "You can execute one of the following functions to get map information (don't execute functions that have been used before):\n",
      "- get_drivable_at_locations(locations) #Get the drivability at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_lane_category_at_locations(locations, return_score) #Get the lane category at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_distance_to_shoulder_at_locations(locations) #Get the distance to both sides of road shoulders at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_current_shoulder() #Get the distance to both sides of road shoulders for the current ego-vehicle location.\n",
      "- get_distance_to_lane_divider_at_locations(locations) #Get the distance to both sides of road lane_dividers at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_current_lane_divider() #Get the distance to both sides of road lane_dividers for the current ego-vehicle location\n",
      "- get_nearest_pedestrian_crossing() #Get the location of the nearest pedestrian crossing to the ego-vehicle. If there is no such pedestrian crossing, return None\n",
      "\n",
      "None\n",
      "\u001b[43mMap Function Call: get_current_shoulder\u001b[0m\n",
      "get_current_shoulder\n",
      "{}\n",
      "Distance to both sides of road shoulders of current ego-vehicle location:\n",
      "Current ego-vehicle's distance to left shoulder is 7.5m and right shoulder is 4.0m\n",
      "\n",
      "*****Perception Results:*****\n",
      "\n",
      "Future trajectories for specific objects:\n",
      "Object type: car, object id: 1, future waypoint coordinates in 3s: [(-14.26, 8.84), (-14.25, 8.83), (-14.23, 8.82), (-14.24, 8.83), (-14.24, 8.83), (-14.22, 8.84)]\n",
      "Object type: car, object id: 2, future waypoint coordinates in 3s: [(4.36, 9.56), (4.36, 9.56), (4.36, 9.57), (4.36, 9.57), (4.36, 9.56), (4.36, 9.56)]\n",
      "Object type: car, object id: 3, future waypoint coordinates in 3s: [(-2.66, 13.82), (-1.69, 14.79), (-0.99, 16.13), (-0.25, 17.73), (0.19, 19.42), (0.57, 21.35)]\n",
      "\n",
      "Distance to both sides of road shoulders of current ego-vehicle location:\n",
      "Current ego-vehicle's distance to left shoulder is 7.5m and right shoulder is 4.0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'red_teaming/RAG_attack/adv_database/memory_cluster_1_2000_adv_instance_0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m perception_agent \u001b[38;5;241m=\u001b[39m PerceptionAgent(token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0a0d6b8c2e884134a3b48df43d54c36a\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_path\u001b[38;5;241m=\u001b[39mdata_path, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m ego_prompts, perception_prompts, working_memory \u001b[38;5;241m=\u001b[39m perception_agent\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m---> 19\u001b[0m memory_agent \u001b[38;5;241m=\u001b[39m \u001b[43mMemoryAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompare_perception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m commonsense_mem, experience_mem \u001b[38;5;241m=\u001b[39m memory_agent\u001b[38;5;241m.\u001b[39mrun(working_memory)\n\u001b[1;32m     22\u001b[0m reasoning_agent \u001b[38;5;241m=\u001b[39m ReasoningAgent(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Agent-Driver/agentdriver/unit_test/../../agentdriver/memory/memory_agent.py:53\u001b[0m, in \u001b[0;36mMemoryAgent.__init__\u001b[0;34m(self, data_path, model_name, verbose, compare_perception, embedding)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience_memory \u001b[38;5;241m=\u001b[39m \u001b[43mExperienceMemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompare_perception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompare_perception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m verbose\n",
      "File \u001b[0;32m~/Agent-Driver/agentdriver/unit_test/../../agentdriver/memory/experience_memory.py:45\u001b[0m, in \u001b[0;36mExperienceMemory.__init__\u001b[0;34m(self, data_path, model_name, verbose, compare_perception, embedding, embedding_model, embedding_tokenizer)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m embedding_model\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_tokenizer \u001b[38;5;241m=\u001b[39m embedding_tokenizer\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_coefs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m10.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[0;32m~/Agent-Driver/agentdriver/unit_test/../../agentdriver/memory/experience_memory.py:101\u001b[0m, in \u001b[0;36mExperienceMemory.load_db\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_sample_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     99\u001b[0m     data_samples \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file) \u001b[38;5;66;03m#[:20000]\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minjected_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    102\u001b[0m     injected_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m    104\u001b[0m new_injected_data \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'red_teaming/RAG_attack/adv_database/memory_cluster_1_2000_adv_instance_0.json'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"/home/czr/Agent-Driver/\")\n",
    "from agentdriver.memory.memory_agent import MemoryAgent\n",
    "from agentdriver.perception.perception_agent import PerceptionAgent\n",
    "from agentdriver.reasoning.reasoning_agent import ReasoningAgent\n",
    "from agentdriver.llm_core.api_keys import OPENAI_ORG, OPENAI_API_KEY\n",
    "\n",
    "import openai\n",
    "openai.organization = OPENAI_ORG\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('../../data/')\n",
    "\n",
    "perception_agent = PerceptionAgent(token=\"0a0d6b8c2e884134a3b48df43d54c36a\", split=\"val\", data_path=data_path, verbose=True)\n",
    "ego_prompts, perception_prompts, working_memory = perception_agent.run()\n",
    "\n",
    "memory_agent = MemoryAgent(data_path=data_path, verbose=True, compare_perception=True)\n",
    "commonsense_mem, experience_mem = memory_agent.run(working_memory)\n",
    "\n",
    "reasoning_agent = ReasoningAgent(verbose=True)\n",
    "reasoning = reasoning_agent.run(perception_agent.data_dict, ego_prompts+perception_prompts, working_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
